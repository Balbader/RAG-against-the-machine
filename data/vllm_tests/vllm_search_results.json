[
  {
    "question": "How does vLLM handle distributed inference?",
    "retrieved_sources": [
      {
        "file_path": "VLLM 0.10.1/vllm-0.10.1/docs/deployment/frameworks/lws.md",
        "first_character_index": 0,
        "last_character_index": 314
      },
      {
        "file_path": "VLLM 0.10.1/vllm-0.10.1/docs/deployment/frameworks/triton.md",
        "first_character_index": 0,
        "last_character_index": 422
      },
      {
        "file_path": "VLLM 0.10.1/vllm-0.10.1/docs/getting_started/installation/gpu/rocm.inc.md",
        "first_character_index": 5424,
        "last_character_index": 5907
      },
      {
        "file_path": "VLLM 0.10.1/vllm-0.10.1/docs/design/plugin_system.md",
        "first_character_index": 313,
        "last_character_index": 948
      },
      {
        "file_path": "VLLM 0.10.1/vllm-0.10.1/examples/offline_inference/qwen2_5_omni/README.md",
        "first_character_index": 0,
        "last_character_index": 130
      }
    ],
    "search_time": 0.11463117599487305
  },
  {
    "question": "What is PagedAttention in vLLM?",
    "retrieved_sources": [
      {
        "file_path": "VLLM 0.10.1/vllm-0.10.1/vllm/attention/backends/rocm_flash_attn.py",
        "first_character_index": 1248,
        "last_character_index": 1994
      },
      {
        "file_path": "VLLM 0.10.1/vllm-0.10.1/benchmarks/README.md",
        "first_character_index": 5295,
        "last_character_index": 5694
      },
      {
        "file_path": "VLLM 0.10.1/vllm-0.10.1/docs/models/supported_models.md",
        "first_character_index": 317,
        "last_character_index": 596
      },
      {
        "file_path": "VLLM 0.10.1/vllm-0.10.1/docs/features/spec_decode.md",
        "first_character_index": 13288,
        "last_character_index": 13766
      },
      {
        "file_path": "VLLM 0.10.1/vllm-0.10.1/vllm/attention/backends/flashinfer.py",
        "first_character_index": 3846,
        "last_character_index": 4017
      }
    ],
    "search_time": 0.09990501403808594
  },
  {
    "question": "How to configure OpenAI compatible server?",
    "retrieved_sources": [
      {
        "file_path": "VLLM 0.10.1/vllm-0.10.1/docs/configuration/serve_args.md",
        "first_character_index": 0,
        "last_character_index": 93
      },
      {
        "file_path": "VLLM 0.10.1/vllm-0.10.1/docs/design/arch_overview.md",
        "first_character_index": 1473,
        "last_character_index": 2156
      },
      {
        "file_path": "VLLM 0.10.1/vllm-0.10.1/docs/features/quantization/bnb.md",
        "first_character_index": 1597,
        "last_character_index": 1745
      },
      {
        "file_path": "VLLM 0.10.1/vllm-0.10.1/vllm/entrypoints/cli/serve.py",
        "first_character_index": 2043,
        "last_character_index": 2635
      },
      {
        "file_path": "VLLM 0.10.1/vllm-0.10.1/docs/getting_started/quickstart.md",
        "first_character_index": 0,
        "last_character_index": 201
      }
    ],
    "search_time": 0.0855855941772461
  },
  {
    "question": "Explain vLLM's quantization support",
    "retrieved_sources": [
      {
        "file_path": "VLLM 0.10.1/vllm-0.10.1/docs/contributing/model/basic.md",
        "first_character_index": 6004,
        "last_character_index": 6111
      },
      {
        "file_path": "VLLM 0.10.1/vllm-0.10.1/docs/features/quantization/supported_hardware.md",
        "first_character_index": 2162,
        "last_character_index": 2698
      },
      {
        "file_path": "VLLM 0.10.1/vllm-0.10.1/vllm/model_executor/layers/quantization/__init__.py",
        "first_character_index": 931,
        "last_character_index": 2352
      },
      {
        "file_path": "VLLM 0.10.1/vllm-0.10.1/docs/getting_started/installation/cpu.md",
        "first_character_index": 9438,
        "last_character_index": 9619
      },
      {
        "file_path": "VLLM 0.10.1/vllm-0.10.1/.buildkite/test-pipeline.yaml",
        "first_character_index": 14000,
        "last_character_index": 16000
      }
    ],
    "search_time": 0.08508753776550293
  },
  {
    "question": "How does continuous batching work in vLLM?",
    "retrieved_sources": [
      {
        "file_path": "VLLM 0.10.1/vllm-0.10.1/docs/README.md",
        "first_character_index": 1336,
        "last_character_index": 3251
      },
      {
        "file_path": "VLLM 0.10.1/vllm-0.10.1/docs/design/plugin_system.md",
        "first_character_index": 0,
        "last_character_index": 313
      },
      {
        "file_path": "VLLM 0.10.1/vllm-0.10.1/docs/features/spec_decode.md",
        "first_character_index": 0,
        "last_character_index": 619
      },
      {
        "file_path": "VLLM 0.10.1/vllm-0.10.1/docs/design/plugin_system.md",
        "first_character_index": 313,
        "last_character_index": 948
      },
      {
        "file_path": "VLLM 0.10.1/vllm-0.10.1/tests/entrypoints/test_chat_utils.py",
        "first_character_index": 10589,
        "last_character_index": 11596
      }
    ],
    "search_time": 0.09073519706726074
  }
]